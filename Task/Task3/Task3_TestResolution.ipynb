{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1533ee8",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The output of the network is given by:\n",
    "\n",
    "$y = \\text{ReLU}(w_1 \\cdot 1 + w_2 \\cdot x_1)$\n",
    "\n",
    "**Given:**\n",
    "- Weight vector: **w = [wâ‚, wâ‚‚] = [4, 1]**\n",
    "- Input: **xâ‚ = 3**\n",
    "- Activation function: **ReLU(x) = max(0, x)**\n",
    "- True label: **y_real = 9**\n",
    "\n",
    "---\n",
    "\n",
    "### a) Network prediction for input xâ‚ = 3\n",
    "\n",
    "Substitute the values into the formula:\n",
    "\n",
    "$z = w_1 \\cdot 1 + w_2 \\cdot x_1 = 4 \\cdot 1 + 1 \\cdot 3 = 4 + 3 = 7$\n",
    "\n",
    "Apply the ReLU activation:\n",
    "\n",
    "$y = \\text{ReLU}(z) = \\max(0, 7) = 7$\n",
    "\n",
    "ğŸ”¸ **Network prediction:** 7\n",
    "\n",
    "---\n",
    "\n",
    "### b) Mean Squared Error (MSE)\n",
    "\n",
    "The quadratic error is computed as:\n",
    "\n",
    "$\\text{Error} = \\frac{1}{2}(y_{\\text{real}} - y_{\\text{predicted}})^2$\n",
    "\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "$\\text{Error} = \\frac{1}{2}(9 - 7)^2 = \\frac{1}{2} \\cdot (2)^2 = \\frac{1}{2} \\cdot 4 = 2$\n",
    "\n",
    "ğŸ”¸ **Mean Squared Error:** \\( \\boxed{2} \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93098580",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "A `MaxPooling2D` layer is initialized in Keras with:\n",
    "\n",
    "layers.MaxPooling2D(pool_size=(3, 3), padding='same', strides=(2, 1))\n",
    "\n",
    "\n",
    "The input tensor `Z` has shape $4 \\times 4$ and is defined as:\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "1 & 0 & 3 & 0 \\\\\n",
    "-5 & 2 & 0 & 4 \\\\\n",
    "3 & 18 & 5 & 0 \\\\\n",
    "1 & 0 & 0 & -2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Computing output shape\n",
    "\n",
    "Given:\n",
    "\n",
    "* Input size: $4 \\times 4$\n",
    "* Kernel size: $3 \\times 3$\n",
    "* Strides: $2, 1$\n",
    "* Padding: `'same'`\n",
    "\n",
    "Using the formula from https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python :\n",
    "\n",
    "$$\n",
    "\\text{output size($H_2$ and $W_2$)} = \\left\\lceil \\frac{\\text{input size}}{\\text{stride}} \\right\\rceil\n",
    "$$\n",
    "\n",
    "* $H_2 = âŒˆ4 / 2âŒ‰ = 2$ \n",
    "* $W_2  = âŒˆ4 / 1âŒ‰ = 4$ \n",
    "---\n",
    "\n",
    "### Applying MaxPooling with padding\n",
    "\n",
    "For `'same'` padding, the output size is:\n",
    "\n",
    "$$\n",
    "\\text{output size} = \\left\\lceil \\frac{\\text{input size}}{\\text{stride}} \\right\\rceil\n",
    "$$\n",
    "\n",
    "The total padding needed is:\n",
    "\n",
    "- **Height padding ($P_h$):**\n",
    "\n",
    "  If $H_1 \\bmod S_h = 0$:\n",
    "\n",
    "  $$\n",
    "  P_h = \\max(F_h - S_h,\\, 0)\n",
    "  $$\n",
    "\n",
    "  Else:\n",
    "\n",
    "  $$\n",
    "  P_h = \\max(F_h - (H_1 \\bmod S_h),\\, 0)\n",
    "  $$\n",
    "\n",
    "- **Width padding ($P_w$):**\n",
    "\n",
    "  If $W_1 \\bmod S_w = 0$:\n",
    "\n",
    "  $$\n",
    "  P_w = \\max(F_w - S_w,\\, 0)\n",
    "  $$\n",
    "\n",
    "  Else:\n",
    "\n",
    "  $$\n",
    "  P_w = \\max(F_w - (W_1 \\bmod S_w),\\, 0)\n",
    "  $$\n",
    "\n",
    "- **Height padding ($P_h$):**\n",
    "  $$\n",
    "  P_h = \\max\\left((3 - 2),\\ 0\\right) = \\max(1,\\ 0) = 1\n",
    "  $$\n",
    "- **Width padding ($P_w$):**\n",
    "  $$\n",
    "  P_w = \\max\\left((3 - 1) ,\\ 0\\right) = \\max(2,\\ 0) = 2\n",
    "  $$\n",
    "\n",
    "**Padding calculation details:**\n",
    "\n",
    "- Top padding ($P_{t}$): $\\left\\lfloor \\dfrac{P_h}{2} \\right\\rfloor = 0$\n",
    "- Bottom padding: $P_h - P_{t} = 1$\n",
    "- Left padding ($P_{l}$): $\\left\\lfloor \\dfrac{P_w}{2} \\right\\rfloor = 1$\n",
    "- Right padding: $P_w - P_{l} = 1$\n",
    "\n",
    "*The bottom and right sides always get the one additional padded pixel if the padding is odd.*\n",
    "\n",
    "The padded input becomes:\n",
    "\n",
    "$$\n",
    "Z_{\\text{padded}} =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 & 3 & 0 & 0 \\\\\n",
    "0 & -5 & 2 & 0 & 4 & 0 \\\\\n",
    "0 & 3 & 18 & 5 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & -2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Computing each window's max\n",
    "\n",
    "We slide a $3 \\times 3$ window with stride (2, 1).\n",
    "\n",
    "#### Row 0:\n",
    "\n",
    "* **(0,0):** max of\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  0 & 1 & 0 \\\\\n",
    "  0 & -5 & 2 \\\\\n",
    "  0 & 3 & 18\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(0,1):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & 3 \\\\\n",
    "  -5 & 2 & 0 \\\\\n",
    "  3 & 18 & 5\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(0,2):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  0 & 3 & 0 \\\\\n",
    "  2 & 0 & 4 \\\\\n",
    "  18 & 5 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(0,3):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  3 & 0 & 0 \\\\\n",
    "  0 & 4 & 0 \\\\\n",
    "  5 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 5\n",
    "  $$\n",
    "\n",
    "#### Row 1:\n",
    "\n",
    "* **(1,0):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  0 & 3 & 18 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(1,1):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  3 & 18 & 5 \\\\\n",
    "  1 & 0 & 0 \\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(1,2):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  18 & 5 & 0 \\\\\n",
    "  0 & 0 & -2 \\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 18\n",
    "  $$\n",
    "\n",
    "* **(1,3):**\n",
    "\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  5 & 0 & 0 \\\\\n",
    "  0 & -2 & 0 \\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  = 5\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Output Tensor:\n",
    "\n",
    "$$\n",
    "\\text{Output} =\n",
    "\\begin{bmatrix}\n",
    "18 & 18 & 18 & 5 \\\\\n",
    "18 & 18 & 18 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Shape:** $(2, 4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8dad17",
   "metadata": {},
   "source": [
    "# Code Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e2ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz Aumentada Z (com padding):\n",
      "[[ -inf  -inf  -inf  -inf  -inf  -inf  -inf]\n",
      " [ -inf    1.  110.    3.   10.  -inf  -inf]\n",
      " [ -inf  170.    2.   34.  -22.  -inf  -inf]\n",
      " [ -inf    3.    1. -255.  220.  -inf  -inf]\n",
      " [ -inf    1.  -23.   45.   27.  -inf  -inf]\n",
      " [ -inf  -inf  -inf  -inf  -inf  -inf  -inf]\n",
      " [ -inf  -inf  -inf  -inf  -inf  -inf  -inf]]\n",
      "Output (shape): (1, 2, 2, 1)\n",
      "Output (values):\n",
      " [[220. 220.]\n",
      " [220. 220.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "# Define the input tensor Z\n",
    "Z = np.array([\n",
    "    [1,  110,  3,  10],\n",
    "    [170, 2,  34,  -22],\n",
    "    [3, 1, -255,  220],\n",
    "    [1,  -23,  45, 27]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Reshape for format: (batch_size, altura, largura, canais)\n",
    "Z = Z.reshape((1, 4, 4, 1))\n",
    "\n",
    "# Define MaxPooling2D layer \n",
    "max_pool = MaxPooling2D(pool_size=(5, 5), strides=(2, 2), padding='same')\n",
    "\n",
    "# Apply max pooling\n",
    "output = max_pool(Z)\n",
    "\n",
    "# --- Visualizando a Matriz Aumentada (com Padding) ---\n",
    "\n",
    "# A lÃ³gica do padding='same' para este caso (entrada 4x4, kernel 5x5, stride 2x2)\n",
    "# requer que a matriz final tenha 5x5 para que a janela do kernel \"caiba\" duas vezes.\n",
    "# Padding Total (Altura): (2-1)*2 + 5 - 4 = 3\n",
    "# Padding Topo = floor(3/2) = 1\n",
    "# Padding Base = 3 - 1 = 2\n",
    "# A mesma lÃ³gica se aplica Ã  largura.\n",
    "\n",
    "# Definindo o padding manualmente para visualizaÃ§Ã£o: [[topo, base], [esquerda, direita]]\n",
    "paddings = tf.constant([[0, 0], [1, 2], [1, 2], [0, 0]]) # Padding na altura e largura\n",
    "\n",
    "# Aplica o padding Ã  matriz Z original\n",
    "Z_padded = tf.pad(Z, paddings, \"CONSTANT\", constant_values=-np.inf)\n",
    "\n",
    "# Mostra a matriz aumentada\n",
    "print(\"\\nMatriz Aumentada Z (com padding):\")\n",
    "print(Z_padded.numpy().squeeze())\n",
    "\n",
    "# Show the output shape and values\n",
    "print(\"Output (shape):\", output.shape)\n",
    "print(\"Output (values):\\n\", output.numpy().squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a2e5a",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The input tensor of a convolutional neural network has shape **(4, 4, 2)**, where:\n",
    "- **4 x 4** = height x width\n",
    "- **2** = depth (channels)\n",
    "\n",
    "The first hidden layer is a **convolutional layer** with:\n",
    "- **2 filters**\n",
    "- **Stride = (1, 2)**\n",
    "- **No padding** (`padding='valid'`)\n",
    "- **Activation: ReLU**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### a)  Number of trainable parameters\n",
    "\n",
    "Each filter has:\n",
    "- Two ($2 \\times 2$) kernels (one per input channel): $2 \\times 2 \\times 2 = 8$\n",
    "- One bias\n",
    "\n",
    "So:\n",
    "- Per filter: **9 parameters**\n",
    "- Total for 2 filters:  \n",
    "\n",
    "$2 \\times (2 \\times 2 \\times 2  + 1) = \\boxed{18 \\text{ trainable parameters}}$\n",
    "\n",
    "---\n",
    "\n",
    "### b) Output tensor shape\n",
    "\n",
    "- Input size: \\(4 \\times 4\\)\n",
    "- Kernel size: \\(2 \\times 2\\)\n",
    "- Stride: (1, 2)\n",
    "- Padding: `'valid'`\n",
    "\n",
    "Using formula from https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python for any zero padding to the input:\n",
    "\n",
    "$$H_2=âŒˆ\\frac{H_1âˆ’F_h+1}{S_h}âŒ‰ = âŒˆ\\frac{4âˆ’2+1}{1}âŒ‰ =3 $$\n",
    "$$W_2=âŒˆ\\frac{W_2âˆ’F_w+1}{S_w}âŒ‰ = âŒˆ\\frac{4âˆ’2+1}{2}âŒ‰ =2 $$\n",
    "\n",
    "- Filters: 2  \n",
    "**Output shape: (3, 2, 2)** (height, width, filters)\n",
    "\n",
    "---\n",
    "\n",
    "### c) Computing output tensor values\n",
    "\n",
    "### Input:\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 2 & 2 \\\\\n",
    "2 & 0 & 0 & 0 \\\\\n",
    "2 & 1 & 0 & 0 \\\\\n",
    "2 & 0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Filters:\n",
    "\n",
    "**Filter 1:**\n",
    "\n",
    "$$\n",
    "F_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "F_2 =\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad \\text{bias} = -3\n",
    "$$\n",
    "\n",
    "**Filter 2:**\n",
    "\n",
    "$$\n",
    "G_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "G_2 =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad \\text{bias} = -5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Stride = (1, 2), Kernel = (2, 2)\n",
    "\n",
    "**The final result below already includes the bias addition and the application of the ReLU activation function to each output element.**\n",
    "\n",
    "**Output shape:** $(3, 2, 2)$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[0, 0]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$0 - 0 + 0 + 1 + (-0 + 0 + 2 + 0) - 3 = 1 + 2 - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$0 + 0 + 0 + 1 + 0 - 5 = \\boxed{0}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[0, 1]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "2 & 2 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$1 + 0 + 0 + 0 + (-2 + 2 + 0 + 0) - 3 = 1 + 0 - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$1 + 0 + 0 + 0 + (4 + 0 + 0 + 0) - 5 = 5 - 5 = \\boxed{0}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[1, 0]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$0 - 1 + 0 + 1 + (-2 + 0 + 2 + 0) - 3 = 0 + 0 - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$0 + 0 + 1 + 1 + (4 + 0 + 0 + 1) - 5 = 2 + 5 - 5 = \\boxed{2}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[1, 1]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$0 - 0 + 0 + 0 + (-0 + 0 + 0 + 0) - 3 =  - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$0 + 0 + 1 + 0 + (0 + 0 + 0 + 0) - 5 = 1 - 5 = \\boxed{0}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[2, 0]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$1 - 1 + 0 + 0 + (-2 + 1 + 2 + 0) - 3 = 0 + 1 - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$1 + 0 + 1 + 0 + 4 - 5 = 6 - 5 = \\boxed{1}$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Output[2, 1]\n",
    "\n",
    "$$\n",
    "X_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X_2 =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Filter 1:**\n",
    "\n",
    "$1 - 0 + 1 + 0 + 0 - 3 = \\boxed{0}$\n",
    "\n",
    "- **Filter 2:**\n",
    "\n",
    "$1 + 0 + 1 + 0 + 2 - 5 = 4 - 5 = \\boxed{0}$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Output Tensor:\n",
    "\n",
    "$$\n",
    "\\text{Output} =\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} & \\begin{bmatrix} 0 & 0 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} & \\begin{bmatrix} 2 & 0 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} 0 & 0 \\end{bmatrix} & \\begin{bmatrix} 1 & 0 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027285d",
   "metadata": {},
   "source": [
    "## Question 4 \n",
    "\n",
    "### Given:\n",
    "\n",
    "Let:\n",
    "\n",
    "- $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ be the input.\n",
    "- Bias is included by prepending a $1$:  \n",
    "  $\\bar{x} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "\n",
    "- Activation function: $\\Phi(x)$ is ReLU:\n",
    "  $$\n",
    "  \\Phi(x) = \\max(0, x)\n",
    "  $$\n",
    "\n",
    "### Weight Matrices:\n",
    "\n",
    "$$\n",
    "W_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & -3 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "-1 & 1 & 1 \\\\\n",
    "1 & -1 & -1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W_2 =\n",
    "\\begin{bmatrix}\n",
    "4 & -5 \\\\\n",
    "-4 & 5 \\\\\n",
    "3 & 1 \\\\\n",
    "2 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### a) Network Topology\n",
    "\n",
    "#### 1. Input Dimensions ($\\bar{x}$)\n",
    "\n",
    "- The operation is $W_1^T \\cdot \\bar{x}$.\n",
    "- $W_1$ has dimension $4 \\times 3$.\n",
    "- Its transpose, $W_1^T$, has dimension $3 \\times 4$.\n",
    "- For the matrix multiplication $(3 \\times 4) \\cdot (\\text{dim of } \\bar{x})$ to be valid, $\\bar{x}$ must be a $4 \\times 1$ vector.\n",
    "- Since $\\bar{x} = [1, x_1, x_2, x_3]^T$, the input vector has 3 features plus 1 bias.\n",
    "\n",
    "**Input dimension:** $4 \\times 1$ (1 for bias + 3 for data).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden Layer Dimensions ($h_1$)\n",
    "\n",
    "- The result of $W_1^T \\cdot \\bar{x}$ is a $(3 \\times 4) \\cdot (4 \\times 1) \\Rightarrow 3 \\times 1$ vector.\n",
    "- This means the first hidden layer has 3 neurons.\n",
    "\n",
    "**Hidden layer dimension:** $4 \\times 1$ (1 for bias + 3 for neurons).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Output Layer Dimensions ($\\bar{o}$)\n",
    "\n",
    "- The input to the second layer is the output vector from the first layer, $h_1$, with bias added. So, the vector feeding the second layer is $4 \\times 1$ (1 for bias + 3 from $h_1$).\n",
    "- The operation is $W_2^T \\cdot h_1$ (with bias).\n",
    "- $W_2$ has dimension $4 \\times 2$.\n",
    "- Its transpose, $W_2^T$, has dimension $2 \\times 4$.\n",
    "- The result of $W_2^T \\cdot h_1$ is a $(2 \\times 4) \\cdot (4 \\times 1) \\Rightarrow 2 \\times 1$ vector.\n",
    "\n",
    "**Output dimension:** $2 \\times 1$.\n",
    "#### 4. Neural Network\n",
    "\n",
    "The network topology diagram was provided in the classroom along with the .ipynb file.\n",
    "\n",
    "---\n",
    "\n",
    "### b) Compute Output for $\\bar{x} = [1, 0, 0, 0]$ \n",
    "\n",
    "$$\n",
    "\\bar{x} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 1: Compute the output of the first hidden layer ($h_1$)**\n",
    "\n",
    "First, calculate the pre-activation value, $z_1$:\n",
    "\n",
    "$$\n",
    "z_1 = W_1^T \\cdot \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "Multiplying:\n",
    "\n",
    "$$\n",
    "z_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & -1 & 1 \\\\\n",
    "1 & -1 & 1 & -1 \\\\\n",
    "-3 & 2 & 1 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "-3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying the ReLU activation function:\n",
    "\n",
    "$$\n",
    "h_1 = \\Phi(z_1) = \\max(0, z_1) =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2: Compute the final output ($\\bar{o}$)**\n",
    "\n",
    "Add the bias to the $h_1$ vector:\n",
    "\n",
    "$$\n",
    "\\bar{h}_1 =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, calculate the pre-activation of the output layer:\n",
    "\n",
    "$$\n",
    "z_2 = W_2^T \\cdot \\bar{h}_1\n",
    "$$\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$\n",
    "W_2^T =\n",
    "\\begin{bmatrix}\n",
    "4 & -4 & 3 & 2 \\\\\n",
    "-5 & 5 & 1 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying:\n",
    "\n",
    "$$\n",
    "z_2 =\n",
    "\\begin{bmatrix}\n",
    "4 & -4 & 3 & 2 \\\\\n",
    "-5 & 5 & 1 & 7\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\cdot 1 + (-4) \\cdot 1 + 3 \\cdot 1 + 2 \\cdot 0 \\\\\n",
    "-5 \\cdot 1 + 5 \\cdot 1 + 1 \\cdot 1 + 7 \\cdot 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying the ReLU activation function:\n",
    "\n",
    "$$\n",
    "\\bar{o} = \\Phi(z_2) = \\max(0, z_2) =\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Final Answer:**  \n",
    "The output of the neural network for an input where all elements are zero ($x_1 = x_2 = x_3 = 0$) is:\n",
    "\n",
    "$$\n",
    "\\bar{o} =\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a299e",
   "metadata": {},
   "source": [
    "# Q  uestion 5\n",
    "### a) Output Tensor Dimension of Each Layer\n",
    "\n",
    "The output dimension of each layer is calculated sequentially, where the output of one layer becomes the input of the next.\n",
    "\n",
    "**Formulas used:**\n",
    "* For Conv2D and MaxPooling2D layers with padding='valid' (no padding): \n",
    "\n",
    "    * $H_{out} = \\left\\lceil \\dfrac{H_{in} - H_{filter}+1}{S_h} \\right\\rceil $\n",
    "    * $W_{out} = \\left\\lceil \\dfrac{W_{in} - W_{filter}+1}{S_w} \\right\\rceil $\n",
    "* For layers with padding='same' (padding to preserve borders):  \n",
    "\n",
    "    * $H_{out} = \\left\\lceil \\dfrac{H_{in}}{S_h} \\right\\rceil$\n",
    "    * $W_{out} = \\left\\lceil \\dfrac{W_{in}}{S_w} \\right\\rceil$\n",
    "\n",
    "**Calculated Dimensions:**\n",
    "\n",
    "1.  **Input Layer**\n",
    "\n",
    "    * Output Shape: (640, 480, 3)\n",
    "\n",
    "2.  **Conv2D (1st Layer)**\n",
    "\n",
    "    * Input: (640, 480, 3)\n",
    "\n",
    "    * Parameters: kernel=(20, 10), strides=(4, 2), padding='valid'\n",
    "\n",
    "    * Height: $H_{out} = \\left\\lceil \\frac{640 - 20 + 1}{4} \\right\\rceil = 156$\n",
    "\n",
    "    * Width: $W_{out} = \\left\\lceil \\frac{480 - 10 + 1}{2} \\right\\rceil= 236$\n",
    "\n",
    "    * Output Shape: (156, 236, 40) (40 is the number of filters)\n",
    "\n",
    "3.  **MaxPooling2D (1st Layer)**\n",
    "\n",
    "    * Input: (156, 236, 40)\n",
    "    * Parameters: pool_size=(2, 2), strides=(2, 2), padding='same'\n",
    "\n",
    "    * Height: $H_{out} = \\left\\lceil \\frac{156}{2} \\right\\rceil = 78$\n",
    "\n",
    "    * Width: $W_{out} = \\left\\lceil \\frac{236}{2} \\right\\rceil = 118$\n",
    "\n",
    "    * Output Shape: (78, 118, 40)\n",
    "\n",
    "4.  **Conv2D (2nd Layer)**\n",
    "\n",
    "    * Input: (78, 118, 40)\n",
    "\n",
    "    * Parameters: kernel=(5, 3), strides=(3, 2), padding='same'\n",
    "\n",
    "    * Height: $H_{out} = \\left\\lceil \\frac{78}{3} \\right\\rceil = 26$\n",
    "\n",
    "    * Width: $W_{out} = \\left\\lceil \\frac{118}{2} \\right\\rceil = 59$\n",
    "\n",
    "    * Output Shape: (26, 59, 6) (6 is the number of filters)\n",
    "\n",
    "5.  **MaxPooling2D (2nd Layer)**\n",
    "\n",
    "* Input: (26, 59, 6)\n",
    "\n",
    "    * Parameters: pool_size=(3, 3), strides=(3, 2), padding='valid'\n",
    "\n",
    "* Height: $H_{out} = \\left\\lceil \\frac{26 - 3 + 1}{3} \\right\\rceil = 8$\n",
    "\n",
    "* Width: $W_{out} = \\left\\lceil \\frac{59 - 3 + 1}{2} \\right\\rceil = 29$\n",
    "\n",
    "* Output Shape: (8, 29, 6)\n",
    "\n",
    "6.  **Flatten**\n",
    "\n",
    "    * Input: (8, 29, 6)\n",
    "    * Operation: $8 \\times 29 \\times 6 = 1392$\n",
    "    * Output Shape: (1392,)\n",
    "\n",
    "7.  **Dense**\n",
    "\n",
    "    * Input: (1392,)\n",
    "    * Output Shape: (5,) (5 is the number of neurons/units)\n",
    "\n",
    "-----\n",
    "### b) Number of Trainable Parameters in Each Layer\n",
    "\n",
    "**Formulas used:**\n",
    "\n",
    "Conv2D: $(H_{kernel} \\times W_{kernel} \\times C_{in} + 1) \\times N_{filters}$ (the `+1` is for the bias of each filter)\n",
    "\n",
    "Dense: $(N_{input} \\times N_{output}) + N_{output}$ (the  N_{output} is for the biases of each neuron)\n",
    "\n",
    "MaxPooling2D and Flatten have no trainable parameters.\n",
    "\n",
    "**Calculated Parameters:**\n",
    "\n",
    "1.  **Conv2D (1st Layer)**\n",
    "\n",
    "    * $(20 \\times 10 \\times 3 + 1) \\times 40 = (600 + 1) \\times 40 = 601 \\times 40 = 24,040$\n",
    "    * **Parameters: 24,040**\n",
    "\n",
    "2.  **MaxPooling2D (1st Layer)**\n",
    "\n",
    "    * **Parameters: 0**\n",
    "\n",
    "3.  **Conv2D (2nd Layer)**\n",
    "\n",
    "    * Input channels come from the previous layer (40 filters).\n",
    "    * $(5 \\times 3 \\times 40 + 1) \\times 6 = (600 + 1) \\times 6 = 601 \\times 6 = 3,606$\n",
    "    * **Parameters: 3,606**\n",
    "\n",
    "4.  **MaxPooling2D (2nd Layer)**\n",
    "\n",
    "    * **Parameters: 0**\n",
    "\n",
    "5.  **Flatten**\n",
    "\n",
    "    * **Parameters: 0**\n",
    "\n",
    "6.  **Dense**\n",
    "\n",
    "    * Input of 1392 neurons from the Flatten layer.\n",
    "    * $(1392 \\times 5) + 5 = 6,960 + 5 = 6,965$\n",
    "    * **Parameters: 6,965**\n",
    "\n",
    "**Total Check:** $24,040 + 3,606 + 6,965 = 34,611$. This matches the total provided in the question.\n",
    "\n",
    "-----\n",
    "\n",
    "### c) Parameters of the 1st Layer with New Input Dimension\n",
    "\n",
    "The formula to calculate the parameters of a Conv2D layer is: \n",
    "\n",
    "$$(H_{kernel} \\times W_{kernel} \\times C_{in} + 1) \\times N_{filters}$$\n",
    "\n",
    "Where:\n",
    "- **Kernel Size** ($H_{kernel}, W_{kernel}$): Defined when creating the layer (20, 10). Does not depend on the input size.\n",
    "- **Input Channels** ($C_{in}$): The depth of the input tensor.\n",
    "- **Number of Filters** ($N_{filters}$): Defined when creating the layer (40). Does not depend on the input size.\n",
    "\n",
    "The spatial dimension of the input (height and width) **does not affect** the number of parameters in a convolutional filter. Only the **depth (number of channels)** of the input matters.\n",
    "\n",
    "- Original input: (640, 480, 3) â†’ $C_{in} = 3$\n",
    "- New input: (6400, 4800, 3) â†’ $C_{in} = 3$\n",
    "  \n",
    "The number of trainable parameters in the first convolutional layer would **remain the same** (**24,040**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745316a",
   "metadata": {},
   "source": [
    "# Code Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c61f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fritz\\miniconda3\\envs\\ic\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">236</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,040</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,606</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1392</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,965</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m236\u001b[0m, \u001b[38;5;34m40\u001b[0m)   â”‚        \u001b[38;5;34m24,040\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m40\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m6\u001b[0m)      â”‚         \u001b[38;5;34m3,606\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m6\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1392\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚         \u001b[38;5;34m6,965\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,611</span> (135.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,611\u001b[0m (135.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,611</span> (135.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,611\u001b[0m (135.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(40, (20, 10), activation='relu', strides=(4,2), input_shape=(640, 480, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same', strides=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(6, (5, 3), activation='relu', padding='same', strides=(3,2)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(3,2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(5, activation='linear'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ba324",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "### **a) What is the value of Ndense?**\n",
    "\n",
    "The formula for the parameters of a dense layer is:\n",
    "$$(\\text{number of input neurons} + 1) \\times \\text{number of output neurons}$$\n",
    "\n",
    "First, we need to know the **number of input neurons**, which is the size of the vector after the Flatten layer. For this, we calculate the output shape after the MaxPooling2D layer.\n",
    "\n",
    "1.  **Input Layer**\n",
    "\n",
    "    * Output Shape: (60, 60, 3)\n",
    "\n",
    "2.  **Conv2D**\n",
    "\n",
    "    * Input: (60, 60, 3)\n",
    "    * Parameters: kernel=(12, 6), strides=(1, 1), padding='valid'\n",
    "    * Height: $H_{out} = \\left\\lceil \\frac{60 - 12 + 1}{1} \\right\\rceil = 49$\n",
    "    * Width: $W_{out} = \\left\\lceil \\frac{60 - 6 + 1}{1} \\right\\rceil= 55$\n",
    "    * Output Shape: (49, 55, 5) \n",
    "\n",
    "3.  **MaxPooling2D**\n",
    "\n",
    "    * Input: (49, 55, 5)\n",
    "    * Parameters: pool_size=(2, 4), strides=(1, 1), padding='same'\n",
    "    * Height: $H_{out} = \\left\\lceil \\frac{49}{1} \\right\\rceil = 49$\n",
    "    * Width: $W_{out} = \\left\\lceil \\frac{55}{1} \\right\\rceil = 55$\n",
    "    * Output Shape: (49, 55, 5)\n",
    "\n",
    "4.  **Flatten**\n",
    "    \n",
    "    * Input: (49, 55, 5)\n",
    "    * Operation: $49 \\times 55 \\times 5 = 13,475$\n",
    "    * Output Shape: (13,475,)\n",
    "\n",
    "5.  **Dense**\n",
    "\n",
    "    * Input: (13,475,)\n",
    "\n",
    "Now, we can define the number of parameters in the Dense layer:  \n",
    "* Input neurons: 13,475  \n",
    "* Output neurons: $N_{dense}$  \n",
    "\n",
    "Dense layer parameters:  \n",
    "$$(13,475 + 1) \\times N_{dense} = 13,476 \\times N_{dense} = 41,513 - 1,085$$\n",
    "$$N_{dense}= \\frac{40,428}{13,476} = 3$$\n",
    "\n",
    "### **b) What is the number of trainable parameters in each layer?**\n",
    "\n",
    "#### **1. Conv2D Layer**\n",
    "\n",
    "The formula to calculate the parameters of a convolutional layer is:\n",
    "$$(\\text{kernel height} \\times \\text{kernel width} \\times \\text{input channels} + 1) \\times \\text{number of filters}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$(12 \\times 6 \\times 3 + 1) \\times 5 = (72 \\times 3 + 1) \\times 5 = (216 + 1) \\times 5 = 217 \\times 5 = \\mathbf{1,085 \\text{ parameters}}$$\n",
    "\n",
    "#### **2. MaxPooling2D and Flatten Layers**\n",
    "\n",
    "The `MaxPooling2D` and `Flatten` layers have no trainable parameters. MaxPooling2D only reduces the spatial dimensionality by applying a max operation, and Flatten only reshapes the data from a multi-dimensional tensor to a one-dimensional vector. Therefore, both have **0 parameters**.\n",
    "\n",
    "#### **3. Dense (Fully Connected) Layer**\n",
    "\n",
    "$$41,513 - 1,085 = 40,428$$\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer         | Output (shape)    | Trainable Parameters |\n",
    "|---------------|-------------------|---------------------|\n",
    "| Conv2D        | (49, 55, 5)       | 1,085               |\n",
    "| MaxPooling2D  | (49, 55, 5)       | 0                   |\n",
    "| Flatten       | (13,475,)         | 0                   |\n",
    "| Dense         | (3,)              | 40,428              |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
